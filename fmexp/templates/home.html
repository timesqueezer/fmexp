<div class="col-xs-12 col-md-10 col-lg-8 mx-auto">
  <div class="bg-white shadow-sm p-3 p-sm-5 article">
    <h2 class="mb-2">Federated Learning and Mouse Data Experiment</h2>

    <p>The goal of this experiment is to collect website browsing behavior data of humans which will be compared against data generated by automated bots. Both metadata of browser requests and some of user inputs (mouse and touch data) will be recorded. It follow a little bit of background on the topic.</p>

    <hr class="my-4">

    <div class="column-2">

      <h4 class="mb-4 column-span-all">On using privacy preseving machine learning for decentralized web bot detection</h4>

      <h4 class="mb-2">Abstract</h4>

      <p>Malicious use of automated bots present an increasing risk to applications in the web. Existing solutions do either not perform well, are not accessible to many providers due to high cost, or disregard modern privacy standards. This work aims to provide a proof-of-concept for a basic system that incorporates all of the above criterions and compares different combinations of the system with and without federated learning and personal data.</p>

      <h4 class="mb-2">Motivation</h4>

      <p>In this work, the term bot is referring to software that is automatically performing HTTP(S) requests with the intent of harming the target or reaching another malicous goal. While this threat is nothing new to the web the attack surface has grown significantly over the past year. Especially the increased usage of web interfaces in poorly secured IoT devices and the trend to (re-)implement software as web applications is responsible for this.</p>

      <p>The usage of bots can have several goals. This thesis primarily focuses on detecting web-based bots that try to access or perform actions on websites but other and related attack types exist, for example:<br>
      DoS attacks aim to overload the target's infrastructure such that it becomes inaccessible for normal use. Carding and Credential stuffing refers to performing payment or login requests to find working credit card numbers and credentials usually obtainend from a data breach. Data scrapers download the website data and can use the data for malicious purposes, e.g. damage SEO or violate copyrights. Content spam includes inserting malicious or polluting data on platforms that allow user generated content. Scalping or inventory hoarding of shopping items can artificially raise prices, damage brands, generate false market forces and create a bad customer experience.</p>

      <p>Recent studies show that of 2020's internet traffic 25.6% was fraudulent and automatically generated. They also show that both the percentage of bot traffic in general and malicious bot traffic has increased over time.</p>

      <p>Most of the above attacks need to trick the webserver and application backends into performing the request as if it had been initiated by a human. Instead of combating the resulting issues separately, bot detection could potentially mitigate many at once.</p>

      <p>A complication in this problem space is the, often desired, requirement for non-malicious bots to be granted normal access. A prominent example are scraper bots used by search engines that need to request websites periodically to build their search indices. A common technique to exploit this requirement is trying to emulate known bot signatures from large search engines, e.g. Googlebot.</p>

      <p>Many website operators tend to use solutions that are easy to integrate. This requires embeding external software which collects user data and sends it to servers of the software vendors. Closed source software does not allow to determine what exactly happens to the user data and website operators open themselves to additional threats in case of a data breach. Depending on the operating countries of both the websites and software vendors, data privacy regulation might also not allow sharing user data at all or require the operator to document the concrete data transfer in a very detailed and legally complicated way, e.g. in countries falling under the GDPR. Because of the above reasons it is desirable to either employ self-hosted software or use a solution that does not require user data transfers.</p>
    </div>
  </div>
</div>
